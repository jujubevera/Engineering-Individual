{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/faculty/.cache/pip/wheels/84/30/e3/c51c5cd0229631e662d29d7b578a3e5949a4c8db033ffb70aa/pyspark-2.4.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: py4j==0.10.7 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pyspark) (0.10.7)\n",
      "Installing collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 2.4.5\n",
      "    Uninstalling pyspark-2.4.5:\n",
      "      Successfully uninstalled pyspark-2.4.5\n",
      "Successfully installed pyspark-2.4.5\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=pyspark-shell>\n",
      "<pyspark.sql.session.SparkSession object at 0x7f6edc2dfda0>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "import pyspark\n",
    "\n",
    "number_cores = 4\n",
    "memory_gb = 16\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster('local[{}]'.format(number_cores))\n",
    "        .set('spark.driver.memory', '{}g'.format(memory_gb))\n",
    ")\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "print(sc)\n",
    "\n",
    "# get the context\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "print(spark) \n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('u.data', sep='\\t', header=None, engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+---------+\n",
      "|user_id|item_id|rating|timestamp|\n",
      "+-------+-------+------+---------+\n",
      "|    196|    242|     3|881250949|\n",
      "|    186|    302|     3|891717742|\n",
      "|     22|    377|     1|878887116|\n",
      "|    244|     51|     2|880606923|\n",
      "|    166|    346|     1|886397596|\n",
      "|    298|    474|     4|884182806|\n",
      "|    115|    265|     2|881171488|\n",
      "|    253|    465|     5|891628467|\n",
      "|    305|    451|     3|886324817|\n",
      "|      6|     86|     3|883603013|\n",
      "|     62|    257|     2|879372434|\n",
      "|    286|   1014|     5|879781125|\n",
      "|    200|    222|     5|876042340|\n",
      "|    210|     40|     3|891035994|\n",
      "|    224|     29|     3|888104457|\n",
      "|    303|    785|     3|879485318|\n",
      "|    122|    387|     5|879270459|\n",
      "|    194|    274|     2|879539794|\n",
      "|    291|   1042|     4|874834944|\n",
      "|    234|   1184|     2|892079237|\n",
      "+-------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=sqlContext.createDataFrame(df)\n",
    "df=df.withColumnRenamed('0','user_id')\n",
    "df=df.withColumnRenamed('1','item_id')\n",
    "df=df.withColumnRenamed('2','rating')\n",
    "df=df.withColumnRenamed('3','timestamp')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- item_id: long (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 4)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(rating)|\n",
      "+-----------+\n",
      "|    3.52986|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Calculate average movie rating\n",
    "df.agg(F.mean('rating')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- item_id: long (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n",
      "79986\n"
     ]
    }
   ],
   "source": [
    "# Test Train Split\n",
    "df.createOrReplaceTempView('df')\n",
    "(training, test) = df.randomSplit([0.8, 0.2])\n",
    "training.printSchema() \n",
    "print(training.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:0.9149176765073985\n",
      "Rank: 30\n",
      "MaxIter: 7\n",
      "RegParam: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rating\",\n",
    "         coldStartStrategy=\"drop\", nonnegative = True)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(als.regParam, [0.03,0.1,0.3]) \\\n",
    "  .addGrid(als.maxIter,[3,5,7]) \\\n",
    "  .addGrid(als.rank, [3,10,30]).build()\n",
    "\n",
    "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=als,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=regEval)\n",
    "\n",
    "model = tvs.fit(training)\n",
    "\n",
    "best_model = model.bestModel\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = best_model.transform(test)\n",
    "rmse = regEval.evaluate(predictions)\n",
    "\n",
    "print(\"RMSE:\"+str(rmse))\n",
    "print(\"Rank:\", best_model.rank)\n",
    "print(\"MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
    "print(\"RegParam:\", best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Flow\n",
    "from collections import deque\n",
    "from six import next\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 4)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_df = pd.read_table('u.data', sep='\\t', header=None, engine = 'python')\n",
    "ts_df.rename(columns={0:'user_id',1:'item_id',2:'rating',3:'timestamp'},inplace=True)\n",
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ts_df.drop('rating',axis=1)\n",
    "y = pd.DataFrame(ts_df['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Recommenders'\n",
      "/project\n"
     ]
    }
   ],
   "source": [
    "#! pip install -e git+https://github.com/microsoft/recommenders/#egg=pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from reco_utils.common.general_utils import get_number_processors\n",
    "from reco_utils.common.gpu_utils import get_cuda_version, get_cudnn_version\n",
    "from reco_utils.dataset.python_splitters import python_stratified_split\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant seed for replicating training results\n",
    "np.random.seed(42)\n",
    "\n",
    "u_num = 6040 # Number of users in the dataset\n",
    "i_num = 3952 # Number of movies in the dataset\n",
    "\n",
    "batch_size = 1000 # Number of samples per batch\n",
    "dims = 5          # Dimensions of the data, 15\n",
    "max_epochs = 50   # Number of times the network sees all the training data\n",
    "\n",
    "# Device used for all computations\n",
    "place_device = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    ts_df = pd.read_table('u.data', sep='\\t', header=None, engine = 'python')\n",
    "    ts_df.rename(columns={0:'user_id',1:'item_id',2:'rating',3:'timestamp'},inplace=True)\n",
    "    rows = len(ts_df)\n",
    "    # Purely integer-location based indexing for selection by position\n",
    "    ts_df = ts_df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    # Separate data into train and test, 90% for train and 10% for test\n",
    "    split_index = int(rows * 0.9)\n",
    "    # Use indices to separate the data\n",
    "    df_train = ts_df[0:split_index]\n",
    "    df_test = ts_df[split_index:].reset_index(drop=True)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(user_batch, item_batch, user_num, item_num, dim=5, device=\"/cpu:0\"):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # Using a global bias term\n",
    "        bias_global = tf.get_variable(\"bias_global\", shape=[])\n",
    "        # User and item bias variables\n",
    "        # get_variable: Prefixes the name with the current variable scope \n",
    "        # and performs reuse checks.\n",
    "        w_bias_user = tf.get_variable(\"embd_bias_user\", shape=[user_num])\n",
    "        w_bias_item = tf.get_variable(\"embd_bias_item\", shape=[item_num])\n",
    "        # embedding_lookup: Looks up 'ids' in a list of embedding tensors\n",
    "        # Bias embeddings for user and items, given a batch\n",
    "        bias_user = tf.nn.embedding_lookup(w_bias_user, user_batch, name=\"bias_user\")\n",
    "        bias_item = tf.nn.embedding_lookup(w_bias_item, item_batch, name=\"bias_item\")\n",
    "        # User and item weight variables\n",
    "        w_user = tf.get_variable(\"embd_user\", shape=[user_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        w_item = tf.get_variable(\"embd_item\", shape=[item_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        # Weight embeddings for user and items, given a batch\n",
    "        embd_user = tf.nn.embedding_lookup(w_user, user_batch, name=\"embedding_user\")\n",
    "        embd_item = tf.nn.embedding_lookup(w_item, item_batch, name=\"embedding_item\")\n",
    "    \n",
    "    with tf.device(device):\n",
    "        # reduce_sum: Computes the sum of elements across dimensions of a tensor\n",
    "        infer = tf.reduce_sum(tf.multiply(embd_user, embd_item), 1)\n",
    "        infer = tf.add(infer, bias_global)\n",
    "        infer = tf.add(infer, bias_user)\n",
    "        infer = tf.add(infer, bias_item, name=\"svd_inference\")\n",
    "        # l2_loss: Computes half the L2 norm of a tensor without the sqrt\n",
    "        regularizer = tf.add(tf.nn.l2_loss(embd_user), tf.nn.l2_loss(embd_item), \n",
    "                             name=\"svd_regularizer\")\n",
    "    return infer, regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(infer, regularizer, rate_batch, learning_rate=0.1, reg=0.1, device=\"/cpu:0\"):\n",
    "    with tf.device(device):\n",
    "        # Use L2 loss to compute penalty\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
    "        penalty = tf.constant(reg, dtype=tf.float32, shape=[], name=\"l2\")\n",
    "        cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))\n",
    "        # 'Follow the Regularized Leader' optimizer\n",
    "        # Reference: http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf\n",
    "        train_op = tf.train.FtrlOptimizer(learning_rate).minimize(cost)\n",
    "    return cost, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples 90000, test samples 10000, samples per batch 90\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = get_data()\n",
    "\n",
    "samples_per_batch = len(df_train) // batch_size\n",
    "print(\"Number of train samples %d, test samples %d, samples per batch %d\" % \n",
    "      (len(df_train), len(df_test), samples_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    877\n",
      "1    815\n",
      "2     94\n",
      "3    416\n",
      "4    500\n",
      "Name: user_id, dtype: int64\n",
      "0    655\n",
      "1    178\n",
      "2    873\n",
      "3     10\n",
      "4    325\n",
      "Name: user_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"user_id\"].head()) \n",
    "print(df_test[\"user_id\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    381\n",
      "1    602\n",
      "2    431\n",
      "3    875\n",
      "4    182\n",
      "Name: item_id, dtype: int64\n",
      "0    271\n",
      "1     83\n",
      "2    294\n",
      "3    116\n",
      "4    305\n",
      "Name: item_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"item_id\"].head())\n",
    "print(df_test[\"item_id\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a shuffle iterator to generate random batches, for training\n",
    "iter_train = readers.ShuffleIterator([df_train[\"user_id\"],\n",
    "                                     df_train[\"item_id\"],\n",
    "                                     df_train[\"rating\"]],\n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "# Sequentially generate one-epoch batches, for testing\n",
    "iter_test = readers.OneEpochIterator([df_test[\"user_id\"],\n",
    "                                     df_test[\"item_id\"],\n",
    "                                     df_test[\"rating\"]],\n",
    "                                     batch_size=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "infer, regularizer = model(user_batch, item_batch, user_num=u_num, item_num=i_num, dim=dims, device=place_device)\n",
    "_, train_op = loss(infer, regularizer, rate_batch, learning_rate=0.10, reg=0.05, device=place_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tTrain Error\tVal Error\tElapsed Time\n",
      "00\t2.376\t\t2.167\t\t0.065 secs\n",
      "01\t1.140\t\t0.960\t\t0.103 secs\n",
      "02\t0.917\t\t0.937\t\t0.097 secs\n",
      "03\t0.892\t\t0.925\t\t0.094 secs\n",
      "04\t0.874\t\t0.919\t\t0.097 secs\n",
      "05\t0.861\t\t0.914\t\t0.096 secs\n",
      "06\t0.855\t\t0.911\t\t0.097 secs\n",
      "07\t0.854\t\t0.910\t\t0.097 secs\n",
      "08\t0.844\t\t0.907\t\t0.095 secs\n",
      "09\t0.839\t\t0.907\t\t0.094 secs\n",
      "10\t0.840\t\t0.906\t\t0.096 secs\n",
      "11\t0.834\t\t0.905\t\t0.095 secs\n",
      "12\t0.826\t\t0.905\t\t0.095 secs\n",
      "13\t0.826\t\t0.904\t\t0.094 secs\n",
      "14\t0.824\t\t0.904\t\t0.096 secs\n",
      "15\t0.817\t\t0.904\t\t0.095 secs\n",
      "16\t0.817\t\t0.904\t\t0.092 secs\n",
      "17\t0.815\t\t0.904\t\t0.097 secs\n",
      "18\t0.814\t\t0.904\t\t0.095 secs\n",
      "19\t0.814\t\t0.904\t\t0.093 secs\n",
      "20\t0.811\t\t0.904\t\t0.096 secs\n",
      "21\t0.809\t\t0.905\t\t0.095 secs\n",
      "22\t0.805\t\t0.905\t\t0.094 secs\n",
      "23\t0.809\t\t0.904\t\t0.098 secs\n",
      "24\t0.803\t\t0.904\t\t0.093 secs\n",
      "25\t0.806\t\t0.904\t\t0.096 secs\n",
      "26\t0.804\t\t0.905\t\t0.094 secs\n",
      "27\t0.800\t\t0.905\t\t0.093 secs\n",
      "28\t0.803\t\t0.905\t\t0.096 secs\n",
      "29\t0.800\t\t0.906\t\t0.096 secs\n",
      "30\t0.799\t\t0.906\t\t0.094 secs\n",
      "31\t0.801\t\t0.906\t\t0.098 secs\n",
      "32\t0.798\t\t0.906\t\t0.095 secs\n",
      "33\t0.793\t\t0.906\t\t0.096 secs\n",
      "34\t0.797\t\t0.907\t\t0.097 secs\n",
      "35\t0.802\t\t0.907\t\t0.095 secs\n",
      "36\t0.799\t\t0.907\t\t0.095 secs\n",
      "37\t0.794\t\t0.907\t\t0.096 secs\n",
      "38\t0.791\t\t0.907\t\t0.096 secs\n",
      "39\t0.792\t\t0.906\t\t0.094 secs\n",
      "40\t0.791\t\t0.906\t\t0.097 secs\n",
      "41\t0.795\t\t0.906\t\t0.096 secs\n",
      "42\t0.792\t\t0.906\t\t0.096 secs\n",
      "43\t0.788\t\t0.906\t\t0.094 secs\n",
      "44\t0.792\t\t0.906\t\t0.094 secs\n",
      "45\t0.789\t\t0.906\t\t0.094 secs\n",
      "46\t0.789\t\t0.906\t\t0.097 secs\n",
      "47\t0.784\t\t0.906\t\t0.107 secs\n",
      "48\t0.791\t\t0.907\t\t0.095 secs\n",
      "49\t0.790\t\t0.907\t\t0.095 secs\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" %(\"Epoch\", \"Train Error\", \"Val Error\", \"Elapsed Time\"))\n",
    "    errors = deque(maxlen=samples_per_batch)\n",
    "    start = time.time()\n",
    "    for i in range(max_epochs*samples_per_batch):\n",
    "        users, items, rates = next(iter_train)\n",
    "        _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch : users,\n",
    "                                                               item_batch : items, \n",
    "                                                               rate_batch : rates})\n",
    "        pred_batch = clip(pred_batch)\n",
    "        errors.append(np.power(pred_batch - rates,2))\n",
    "        if i % samples_per_batch == 0:\n",
    "            train_err = np.sqrt(np.mean(errors))\n",
    "            test_err2 = np.array([])\n",
    "            for users, items, rates in iter_test:\n",
    "                pred_batch = sess.run(infer, feed_dict={user_batch : users,\n",
    "                                                        item_batch : items})\n",
    "                pred_batch = clip(pred_batch)\n",
    "                test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "            end = time.time()\n",
    "            print(\"%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\" % (i // samples_per_batch, train_err, np.sqrt(np.mean(test_err2)), end - start))\n",
    "            start = end\n",
    "\n",
    "    saver.save(sess, './save/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./save/model\n",
      "Pred\tActual\n",
      "2.426\t3.000\n",
      "3.389\t5.000\n",
      "3.669\t3.000\n",
      "3.693\t4.000\n",
      "4.372\t5.000\n",
      "3.950\t5.000\n",
      "2.730\t2.000\n",
      "3.791\t4.000\n",
      "3.387\t3.000\n",
      "3.750\t5.000\n",
      "0.9071531696822109\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(init_op)\n",
    "    new_saver = tf.train.import_meta_graph('./save/model.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./save/'))\n",
    "    test_err2 = np.array([])\n",
    "    for users, items, rates in iter_test:\n",
    "        pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                item_batch: items})\n",
    "        pred_batch = clip(pred_batch)\n",
    "        print(\"Pred\\tActual\")\n",
    "        for ii in range(10):\n",
    "            print(\"%.3f\\t%.3f\" % (pred_batch[ii], rates[ii]))\n",
    "        test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "        print(\"RMSE:\",np.sqrt(np.mean(test_err2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (5.3)\n",
      "Requirement already satisfied: h5py in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (1.14.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Embedding, Reshape, Dropout, Dense, merge, Input, Flatten, Concatenate, Dot, Add, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0      196      242       3\n",
       "1      186      302       3\n",
       "2       22      377       1\n",
       "3      244       51       2\n",
       "4      166      346       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "nn_df = pd.read_table('u.data', sep='\\t', header=None, engine = 'python')\n",
    "nn_df.rename(columns={0:'user_id',1:'item_id',2:'rating',3:'timestamp'},inplace=True)\n",
    "num_users = nn_df['user_id'].max()\n",
    "num_items = nn_df['item_id'].max()\n",
    "nn_df = nn_df.drop('timestamp', 1)\n",
    "nn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling data\n",
    "shuffled_df = nn_df.sample(frac=1., random_state=42)\n",
    "users = shuffled_df['user_id'].values\n",
    "items = shuffled_df['item_id'].values\n",
    "ratings = shuffled_df['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.astype('float')\n",
    "items = items.astype('float')\n",
    "ratings = ratings.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = shuffled_df.user_id.max() + 1 # number of users\n",
    "M = shuffled_df.item_id.max() + 1 # number of movies\n",
    "\n",
    "# initialize variables\n",
    "K = 10 # latent dimensionality\n",
    "mu = shuffled_df.rating.mean()\n",
    "epochs = 15\n",
    "reg = 0. # regularization penalty\n",
    "\n",
    "# keras model\n",
    "u = Input(shape=(1,))\n",
    "m = Input(shape=(1,))\n",
    "u_embedding = Embedding(N, K)(u) # (N, 1, K)\n",
    "m_embedding = Embedding(M, K)(m) # (N, 1, K)\n",
    "\n",
    "# main branch\n",
    "u_bias = Embedding(N, 1)(u) # (N, 1, 1)\n",
    "m_bias = Embedding(M, 1)(m) # (N, 1, 1)\n",
    "x = Dot(axes=2)([u_embedding, m_embedding]) # (N, 1, 1)\n",
    "x = Add()([x, u_bias, m_bias])\n",
    "x = Flatten()(x) # (N, 1)\n",
    "\n",
    "# side branch\n",
    "u_embedding = Flatten()(u_embedding) # (N, K)\n",
    "m_embedding = Flatten()(m_embedding) # (N, K)\n",
    "y = Concatenate()([u_embedding, m_embedding]) # (N, 2K)\n",
    "y = Dense(400)(y)\n",
    "y = Activation('elu')(y)\n",
    "# y = Dropout(0.5)(y)\n",
    "y = Dense(1)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda/envs/Python3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "90000/90000 [==============================] - 1s 14us/step - loss: 0.9910 - mse: 0.9910 - val_loss: 0.9301 - val_mse: 0.9301\n",
      "Epoch 2/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.8912 - mse: 0.8912 - val_loss: 0.8949 - val_mse: 0.8949\n",
      "Epoch 3/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.8721 - mse: 0.8721 - val_loss: 0.8906 - val_mse: 0.8906\n",
      "Epoch 4/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.8589 - mse: 0.8589 - val_loss: 0.8976 - val_mse: 0.8976\n",
      "Epoch 5/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.8449 - mse: 0.8449 - val_loss: 0.8917 - val_mse: 0.8917\n",
      "Epoch 6/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.8328 - mse: 0.8328 - val_loss: 0.8670 - val_mse: 0.8670\n",
      "Epoch 7/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.8205 - mse: 0.8205 - val_loss: 0.8626 - val_mse: 0.8626\n",
      "Epoch 8/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.8096 - mse: 0.8096 - val_loss: 0.8566 - val_mse: 0.8566\n",
      "Epoch 9/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.7999 - mse: 0.7999 - val_loss: 0.8535 - val_mse: 0.8535\n",
      "Epoch 10/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.7858 - mse: 0.7858 - val_loss: 0.8576 - val_mse: 0.8576\n",
      "Epoch 11/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.7736 - mse: 0.7736 - val_loss: 0.8400 - val_mse: 0.8400\n",
      "Epoch 12/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.7643 - mse: 0.7643 - val_loss: 0.8466 - val_mse: 0.8466\n",
      "Epoch 13/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.7464 - mse: 0.7464 - val_loss: 0.8437 - val_mse: 0.8437\n",
      "Epoch 14/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.7301 - mse: 0.7301 - val_loss: 0.8341 - val_mse: 0.8341\n",
      "Epoch 15/15\n",
      "90000/90000 [==============================] - 1s 11us/step - loss: 0.7135 - mse: 0.7135 - val_loss: 0.8368 - val_mse: 0.8368\n"
     ]
    }
   ],
   "source": [
    "# merge\n",
    "x = Add()([x, y])\n",
    "\n",
    "model = Model(inputs=[u, m], outputs=x)\n",
    "model.compile(\n",
    "  loss='mse',\n",
    "  # optimizer='adam',\n",
    "  # optimizer=Adam(lr=0.01),\n",
    "  optimizer=SGD(lr=0.08, momentum=0.9),\n",
    "  metrics=['mse'],)\n",
    "r = model.fit(\n",
    "  x=[users, items],y=ratings - mu,\n",
    "  epochs=epochs,\n",
    "  batch_size=150,\n",
    "  validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_mse', 'loss', 'mse'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:0.816\n"
     ]
    }
   ],
   "source": [
    "print('MSE:{:.3f}'.format(np.mean(r.history['mse'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 3.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAe+0lEQVR4nO3dfXRU9b3v8fd3HpKQEB4EVAQUbK1akCej0kurKNqF1mprqcU+nOJtS0vbY+3tvUftOUeqa/Uszzpej/W26kFra1uWysHacnrVWlusuq5aA0VEsZVWrBFFQCUoSZiH7/1j7wyTyUwSIDuTsD+vtWbN3vv323u+SWA+89t75jfm7oiISHwlql2AiIhUl4JARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiLrIgMLM6M/uDmT1jZs+Z2TVl+tSa2T1mttnMnjKzyVHVIyIi5UU5IugAznL3GcBMYIGZzSnp8wXgLXd/L/DvwL9GWI+IiJQRWRB44J1wNR3eSj+9diFwZ7i8CphvZhZVTSIi0l0qyoObWRJYC7wX+IG7P1XSZQLwCoC7Z81sFzAG2FFynCXAEoCGhoaTTzjhhCjLFhE55Kxdu3aHu48r1xZpELh7DphpZqOA+8xsmrtvLOpS7tV/tzkv3H05sBygqanJm5ubI6lXRORQZWYvV2obkHcNufvbwCPAgpKmFmASgJmlgJHAmwNRk4iIBKJ819C4cCSAmQ0DzgZeKOm2Gvh8uLwQ+J1rFjwRkQEV5amh8cCd4XWCBLDS3X9lZtcCze6+Gvgh8FMz20wwElgUYT0iIlJGZEHg7huAWWW2X1203A58MqoaRGTwy2QytLS00N7eXu1SDgl1dXVMnDiRdDrd530ivVgsItKblpYWGhsbmTx5Mnr3+MFxd3bu3ElLSwtTpkzp836aYkJEqqq9vZ0xY8YoBPqBmTFmzJj9Hl0pCESk6hQC/edAfpcKAhGRmFMQiEisvf3229x88837vd95553H22+/3WOfq6++mocffvhASxswCgIRibVKQZDL5Xrc7/7772fUqFE99rn22ms5++yzD6q+gaAgEJFYu/LKK/nLX/7CzJkzOeWUUzjzzDP59Kc/zUknnQTAxz72MU4++WSmTp3K8uXLC/tNnjyZHTt2sGXLFk488US+9KUvMXXqVD784Q/T1tYGwOLFi1m1alWh/7Jly5g9ezYnnXQSL7wQfL52+/btnHPOOcyePZsvf/nLHHPMMezYsYOBpLePisigcc1/PcfzW1v79ZjvP2oEyz46tWL7ddddx8aNG1m/fj2PPPIIH/nIR9i4cWPh7Zd33HEHhx12GG1tbZxyyil84hOfYMyYMV2O8eKLL3LXXXdx2223cfHFF3Pvvffy2c9+tttjjR07lnXr1nHzzTdz/fXXc/vtt3PNNddw1llncdVVV/Hggw92CZuBohGBiEiRU089tct78G+66SZmzJjBnDlzeOWVV3jxxRe77TNlyhRmzpwJwMknn8yWLVvKHvuiiy7q1ufxxx9n0aJgUoUFCxYwevTofvxp+kYjAhEZNHp65T5QGhoaCsuPPPIIDz/8ME888QT19fXMmzev7Hv0a2trC8vJZLJwaqhSv2QySTabBYIPgVWbRgQiEmuNjY3s3r27bNuuXbsYPXo09fX1vPDCCzz55JP9/vgf/OAHWblyJQAPPfQQb731Vr8/Rm80IhCRWBszZgxz585l2rRpDBs2jCOOOKLQtmDBAm699VamT5/O8ccfz5w5pd+2e/CWLVvGJZdcwj333MMZZ5zB+PHjaWxs7PfH6YkNhmHJ/tAX04gcWjZt2sSJJ55Y7TKqpqOjg2QySSqV4oknnmDp0qWsX7/+oI5Z7ndqZmvdvalcf40IRESq6G9/+xsXX3wx+XyempoabrvttgGvQUEgIlJFxx13HH/84x+rWoMuFouIxJyCQEQk5hQEIiIxpyAQEYk5BYGIyH4YPnw4AFu3bmXhwoVl+8ybN4/e3uZ+4403smfPnsJ6X6a1joqCQETkABx11FGFmUUPRGkQ9GVa66goCEQk1q644oou30fwne98h2uuuYb58+cXpoz+5S9/2W2/LVu2MG3aNADa2tpYtGgR06dP51Of+lSXuYaWLl1KU1MTU6dOZdmyZUAwkd3WrVs588wzOfPMM4F901oD3HDDDUybNo1p06Zx4403Fh6v0nTXB0ufIxCRweOBK+H1Z/v3mEeeBOdeV7F50aJFXH755Xz1q18FYOXKlTz44IN885vfZMSIEezYsYM5c+ZwwQUXVPw+4FtuuYX6+no2bNjAhg0bmD17dqHtu9/9Locddhi5XI758+ezYcMGLrvsMm644QbWrFnD2LFjuxxr7dq1/OhHP+Kpp57C3TnttNM444wzGD16dJ+nu95fGhGISKzNmjWLN954g61bt/LMM88wevRoxo8fz7e//W2mT5/O2Wefzauvvsq2bdsqHuPRRx8tPCFPnz6d6dOnF9pWrlzJ7NmzmTVrFs899xzPP/98j/U8/vjjfPzjH6ehoYHhw4dz0UUX8dhjjwF9n+56f2lEICKDRw+v3KO0cOFCVq1axeuvv86iRYtYsWIF27dvZ+3ataTTaSZPnlx2+uli5UYLL730Etdffz1PP/00o0ePZvHixb0ep6f53/o63fX+0ohARGJv0aJF3H333axatYqFCxeya9cuDj/8cNLpNGvWrOHll1/ucf/TTz+dFStWALBx40Y2bNgAQGtrKw0NDYwcOZJt27bxwAMPFPapNP316aefzi9+8Qv27NnDu+++y3333ceHPvShfvxpu9OIQERib+rUqezevZsJEyYwfvx4PvOZz/DRj36UpqYmZs6cyQknnNDj/kuXLuXSSy9l+vTpzJw5k1NPPRWAGTNmMGvWLKZOncqxxx7L3LlzC/ssWbKEc889l/Hjx7NmzZrC9tmzZ7N48eLCMb74xS8ya9asfjsNVE5k01Cb2STgJ8CRQB5Y7u7fK+kzD/gl8FK46efufm1Px9U01CKHlrhPQx2FwTQNdRb4lruvM7NGYK2Z/cbdS6+UPObu50dYh4iI9CCyawTu/pq7rwuXdwObgAlRPZ6IiByYAblYbGaTgVnAU2WaP2Bmz5jZA2ZW/W+uFpEBN9S+KXEwO5DfZeRBYGbDgXuBy929taR5HXCMu88A/g/wiwrHWGJmzWbWvH379mgLFpEBVVdXx86dOxUG/cDd2blzJ3V1dfu1X6TfWWxmaeBXwK/d/YY+9N8CNLn7jkp9dLFY5NCSyWRoaWnp9f310jd1dXVMnDiRdDrdZXtVLhZb8OmKHwKbKoWAmR0JbHN3N7NTCUYoO6OqSUQGn3Q6zZQpU6pdRqxF+a6hucDngGfNbH247dvA0QDufiuwEFhqZlmgDVjkGh+KiAyoyILA3R8Hys/QtK/P94HvR1WDiIj0TlNMiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARibnIgsDMJpnZGjPbZGbPmdk3yvQxM7vJzDab2QYzmx1VPSIiUl4qwmNngW+5+zozawTWmtlv3P35oj7nAseFt9OAW8J7EREZIJGNCNz9NXdfFy7vBjYBE0q6XQj8xANPAqPMbHxUNYmISHcDco3AzCYDs4CnSpomAK8UrbfQPSwwsyVm1mxmzdu3b4+qTBGRWIo8CMxsOHAvcLm7t5Y2l9nFu21wX+7uTe7eNG7cuCjKFBGJrUiDwMzSBCGwwt1/XqZLCzCpaH0isDXKmkREpKso3zVkwA+BTe5+Q4Vuq4G/C989NAfY5e6vRVWTiIh0F+W7huYCnwOeNbP14bZvA0cDuPutwP3AecBmYA9waYT1iIhIGZEFgbs/TvlrAMV9HPhaVDWIiEjv9MliEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5noMAjM7q2h5SknbRVEVJSIiA6e3EcH1Rcv3lrT9Uz/XIiIiVdBbEFiF5XLrIiIyBPUWBF5hudy6iIgMQale2o81s9UEr/47lwnXp1TeTUREhoreguDCouXrS9pK10VEZAjqMQjc/ffF62aWBqYBr7r7G1EWJiIiA6O3t4/eamZTw+WRwDPAT4A/mtklA1CfiIhErLeLxR9y9+fC5UuBP7v7ScDJwD/0tKOZ3WFmb5jZxgrt88xsl5mtD29X73f1IiJy0Hq7RrC3aPkc4D8B3P11s17fPfpj4PsEI4hKHnP383s7kIiIRKe3EcHbZna+mc0C5gIPAphZChjW047u/ijwZr9UKSIikektCL4MfB34EXC5u78ebp8P/N9+ePwPmNkzZvZA57WIcsxsiZk1m1nz9u3b++FhRUSkk7lH97kwM5sM/Mrdp5VpGwHk3f0dMzsP+J67H9fbMZuamry5ubnfaxUROZSZ2Vp3byrX1uM1AjO7qad2d7/sQIty99ai5fvN7GYzG+vuOw70mCIisv96u1j8FWAjsBLYSj/OL2RmRwLb3N3N7FSC01Q7++v4IiLSN70FwXjgk8CngCxwD3Cvu7/V24HN7C5gHjDWzFqAZUAawN1vBRYCS80sC7QBizzK81QiIlJWn68RmNkE4BLgfwBXuPtPoyysEl0jEBHZfwd8jaDoALMJQuAc4AFgbf+VJyIi1dTbxeJrgPOBTcDdwFXunh2IwkREZGD0NiL4Z+CvwIzw9i/hJ4oNcHefHm15IiIStd6CQN85ICJyiOttGuqXy203sySwCCjbLiIiQ0dv01CPMLOrzOz7ZvZhC/w9wemiiwemRBERiVJvp4Z+CrwFPAF8EfhfQA1wobuvj7g2EREZAL1+Z3H4/QOY2e3ADuBod98deWUiIjIgept9NNO54O454CWFgIjIoaW3EcEMM+ucHM6AYeF659tHR0RanYiIRK63dw0lB6oQERGpjt5ODYmIyCFOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMxFFgRmdoeZvWFmGyu0m5ndZGabzWyDmc2OqhYREaksyhHBj4EFPbSfCxwX3pYAt0RYi4iIVBBZELj7o8CbPXS5EPiJB54ERpnZ+Kjqebcjy7bWdtw9qocQERmSevvy+ihNAF4pWm8Jt71W2tHMlhCMGjj66KMP6MEee3E7X/nZOkYOS3P8EY2878jhHH9EI8cd0cjxRzQyuqHmgI4rIjLUVTMIrMy2si/X3X05sBygqanpgF7STz1qJNdeOJU/vb6bP2/bzer1W2ltzxbaxzXWBgFxRCPHHzmc94UhMby2mr8iEZHoVfNZrgWYVLQ+Edga1YNNevP/8XfNV0DdSKgfgb9/JG2J4ezMDeP1jhpe2ZPmpbdTvPhykg3ZWlppoNXrGT5yDEcfOY73HTmiEBDvGTecunQyqlJFRAZUNYNgNfB1M7sbOA3Y5e7dTgv1m9qRMH4GtO+C9l1Y66vUt7dS376LSdk2TunslwxvnTog93KC1i31tHo9rdSzjnpyNSNIDBtFXeNh1Dc0kkwmSSaTpIruU8kEyWSKVCpYT6eCNrMkmIW3ROWb5yGfA88F9/ls922Ftj5u8zxgkEiAJSGRDB+vc9mKlsO2Qp/Evu2l2xIpSNdDTUP3+87l9LDg+CIyqEQWBGZ2FzAPGGtmLcAyIA3g7rcC9wPnAZuBPcClUdUCwKRTgls52Q5ob4WOVmh/OwyL1kJoJNt3MaJ9F8ldO6lrfZPR774FHdtIv7OZ+tZ3qacdAxJW/QvReUvh4RO7d3uSD+4NxzwH3nmfw9y7B4bn+rk6KwqHekiH9zUN+5ZLwyNVVxSOvQSnJcKA6+lWfIzioEsW7VshCLttK7MP1vVx6Az8AQxA9/DvV+kWtlsi+P2mahXQMRdZELj7Jb20O/C1qB5/v6RqYfi44FZBEhgR3oq1Z3K0tLbTnsnTkcnSnsnRvjfD3kyWjkyWjkyGjkyOvZkMe7M5OjJZMpkcHZkMe7NZMtmwLZMjk82yN5sjk8mSzWXpyMLePHTkoCOfoD0HbVnIkSBPost9jgR+EG8CSyWMmlSCdDJBTSpBTed9wqlNGbVJqEs6tUmjNunUJqAmCbVJqE06dYk8w9jLMDqop5062qnzdmo9uK/JB8s1+TbSuTbS+TbSuXZS2T2kOlpJZl8nmWsjmW0jkXkXy+zByl8yGsLKhUS43qXNureVPonTw5P9gUjWQrpuXzB0uS/dXleh77DgHrq+oCi8wMiW2Zar0Ld0BBz+XN1+XyW/1z4t0/XvkEgHI9pkKrhPpMJtSUim+7hedEum943o3cO/VfHfrXNbcTu9tIfroybBYcce2N+4B7oSepDq0kmOGdMwYI/n7uTyTibn7M3lyeTy7M0G98Gy71vO5cnknEx23/rebJ5s3gv7dGS7HmNvNs/enHdZL973nUyeTHvndg/7h8fNJcjk6tibq6F7ZO73T0otGWrZG4y2yIdR59QknZqkUZMIQqhzOZ10ahJdl9MJSCecdBhc6QSkzUkVtjtpC27JcDll+cJ9KgFJ8qTC9aQR3JMnaXlSFtSWIk/C8iQJRoZJg0TYFtw7CXMSGAmKn8h935Nctyf5onUrOhVXHBh9Hv0kSsInETzhZjvCW1t4377vPtO+b33Pzsr98tlKf8ReWMmpxs4RVniqsXTUVfgdVXhyrbjcQ3/PQz5zwP9KB9zcy+Gca/r9sAqCIcbMSCWNVBKGdbmYMXgUh1Umnw+DaF9AlS5ni0Krp+UgkIJ9s+H2vbng+Nn8vuU9ua5hl8mUe2wn21lbPjh+foAGIGaQTiRIJoK/ZToZLKcTRiqZIBVuTyUS4X0wWqtJJYPQKxqxFUZwqQS1ya7rXUZ2RdtLR33p8DE7l9NhTTXJBIlEH04Z5bJFwdAW/pAlT+6JVLdTlIPqdFTnCCSX2TdyyWeKtvVlPbtvWz5feXRXetqwWzuV20dMiOTHVxBIvxsKYVVOPh8EV7YkLDrXs3nvEl6l7cXrufBYnYGYDfcP2vaFT+e2bPF6l7Z9gdfaluk2Qivcwm39/XnJZMIKwVAaEulkgnRqX1tNMlEIsHTSwn0ThWMkE2G4dQu8ovWEkUwmSCe671+bSlKbTlCXTlKXSlKXTlCbTlKXCrelkyT7ElzlJMJw6jy1FTMKApFQImHUJpIM1Y+OuHthJFR86q84NDpP93UUTud50WnE0tFV6WnGom35rqccM7k8bZkcmfYguHJhIFYMuIhGYemkURcGRm0YFp0hUVsIjETYJwyTVNBWW7ycCkKmsFw4ZqJwrOJtNckENphGOPtpiP6TF5FSZvtevTcMkRe2+TAUckWjsS6jqlwwquoMs/ZMnvZsjo5MLljO5OjIBvedbZ3LHdkcHWGfYHueXW2ZLv07MrlCKB6scgFSHBqF0CkEUXF7uYAp6ZtKMq6xlnGN/f/HVRCISNUkEkZNeDqnmqcR82HYdHQGSDa4DwKlaFuX9jBIskV9MqXb94XSjneyhX07A6wjDK++ntL7yhnv4cpzT+j3n19BICKxl0gYdYlkOGNAekAf2z24jlQcGsVhURwak8fWR1KDgkBEpIrMjJpU8K6txrrq1KBvKBMRiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGIu0iAwswVm9icz22xmV5ZpX2xm281sfXj7YpT1iIhId5F9eb2ZJYEfAOcALcDTZrba3Z8v6XqPu389qjpERKRnUY4ITgU2u/tf3X0vcDdwYYSPJyIiByDKIJgAvFK03hJuK/UJM9tgZqvMbFKE9YiISBlRBoGV2eYl6/8FTHb36cDDwJ1lD2S2xMyazax5+/bt/VymiEi8RRkELUDxK/yJwNbiDu6+0907wtXbgJPLHcjdl7t7k7s3jRs3LpJiRUTiKsogeBo4zsymmFkNsAhYXdzBzMYXrV4AbIqwHhERKSOydw25e9bMvg78GkgCd7j7c2Z2LdDs7quBy8zsAiALvAksjqoeEREpz9xLT9sPbk1NTd7c3FztMkREhhQzW+vuTeXa9MliEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMRdpEJjZAjP7k5ltNrMry7TXmtk9YftTZjY5ynpERKS7yILAzJLAD4BzgfcDl5jZ+0u6fQF4y93fC/w78K9R1SMiIuVFOSI4Fdjs7n91973A3cCFJX0uBO4Ml1cB883MIqxJRERKpCI89gTglaL1FuC0Sn3cPWtmu4AxwI7iTma2BFgSrr5jZn+KpOIDN5aSmge5oVTvUKoVhla9Q6lWGFr1DsZaj6nUEGUQlHtl7wfQB3dfDizvj6KiYGbN7t5U7Tr6aijVO5RqhaFV71CqFYZWvUOpVoj21FALMKlofSKwtVIfM0sBI4E3I6xJRERKRBkETwPHmdkUM6sBFgGrS/qsBj4fLi8Efufu3UYEIiISnchODYXn/L8O/BpIAne4+3Nmdi3Q7O6rgR8CPzWzzQQjgUVR1ROxQXvaqoKhVO9QqhWGVr1DqVYYWvUOpVoxvQAXEYk3fbJYRCTmFAQiIjGnIDgIZjbJzNaY2SYze87MvlHtmnpjZkkz+6OZ/aratfTGzEaZ2SozeyH8HX+g2jVVYmbfDP8NbDSzu8ysrto1FTOzO8zsDTPbWLTtMDP7jZm9GN6PrmaNnSrU+m/hv4MNZnafmY2qZo3FytVb1PY/zczNbGw1ausrBcHByQLfcvcTgTnA18pMozHYfAPYVO0i+uh7wIPufgIwg0Fat5lNAC4Dmtx9GsGbIwbbGx9+DCwo2XYl8Ft3Pw74bbg+GPyY7rX+Bpjm7tOBPwNXDXRRPfgx3evFzCYB5wB/G+iC9peC4CC4+2vuvi5c3k3wRDWhulVVZmYTgY8At1e7lt6Y2QjgdIJ3luHue9397epW1aMUMCz8PEw93T8zU1Xu/ijdP6NTPMXLncDHBrSoCsrV6u4PuXs2XH2S4HNJg0KF3y0E86f9A2U+JDvYKAj6SThz6izgqepW0qMbCf5h5qtdSB8cC2wHfhSeyrrdzBqqXVQ57v4qcD3BK7/XgF3u/lB1q+qTI9z9NQhe1ACHV7mevvrvwAPVLqInZnYB8Kq7P1PtWvpCQdAPzGw4cC9wubu3VruecszsfOANd19b7Vr6KAXMBm5x91nAuwyeUxddhOfWLwSmAEcBDWb22epWdWgys38kOCW7otq1VGJm9cA/AldXu5a+UhAcJDNLE4TACnf/ebXr6cFc4AIz20IwE+xZZvaz6pbUoxagxd07R1irCIJhMDobeMndt7t7Bvg58N+qXFNfbDOz8QDh/RtVrqdHZvZ54HzgM4N8BoL3ELwoeCb8/zYRWGdmR1a1qh4oCA5COGX2D4FN7n5Dtevpibtf5e4T3X0ywYXM37n7oH3V6u6vA6+Y2fHhpvnA81UsqSd/A+aYWX34b2I+g/TCdoniKV4+D/yyirX0yMwWAFcAF7j7nmrX0xN3f9bdD3f3yeH/txZgdvhvelBSEBycucDnCF5drw9v51W7qEPI3wMrzGwDMBP4lyrXU1Y4alkFrAOeJfh/NaimGDCzu4AngOPNrMXMvgBcB5xjZi8SvLvlumrW2KlCrd8HGoHfhP/Pbq1qkUUq1DukaIoJEZGY04hARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgMoDMbN5QmPlV4kVBICIScwoCkTLM7LNm9ofww0v/EX6Pwztm9r/NbJ2Z/dbMxoV9Z5rZk0Vz5Y8Ot7/XzB42s2fCfd4THn540fcsrAg/jSxSNQoCkRJmdiLwKWCuu88EcsBngAZgnbvPBn4PLAt3+QlwRThX/rNF21cAP3D3GQRzD70Wbp8FXA68n2CW1bmR/1AiPUhVuwCRQWg+cDLwdPhifRjBhGx54J6wz8+An5vZSGCUu/8+3H4n8J9m1ghMcPf7ANy9HSA83h/cvSVcXw9MBh6P/scSKU9BINKdAXe6e5dvwTKzfy7p19P8LD2d7ukoWs6h/4dSZTo1JNLdb4GFZnY4FL7b9xiC/y8Lwz6fBh53913AW2b2oXD754Dfh99L0WJmHwuPURvOUy8y6OiViEgJd3/ezP4JeMjMEkAG+BrBl+NMNbO1wC6C6wgQTOF8a/hE/1fg0nD754D/MLNrw2N8cgB/DJE+0+yjIn1kZu+4+/Bq1yHS33RqSEQk5jQiEBGJOY0IRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5v4/3OlKIjJ8cLIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame({'epoch': [ i + 1 for i in r.epoch ],\n",
    "                     'training': [ math.sqrt(loss) for loss in r.history['loss'] ],\n",
    "                     'validation': [ math.sqrt(loss) for loss in r.history['val_loss'] ]})\n",
    "ax = results.iloc[:,:].plot(x='epoch')\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.set_ylim([0.0,3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
