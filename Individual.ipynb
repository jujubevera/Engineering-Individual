{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/faculty/.cache/pip/wheels/84/30/e3/c51c5cd0229631e662d29d7b578a3e5949a4c8db033ffb70aa/pyspark-2.4.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: py4j==0.10.7 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pyspark) (0.10.7)\n",
      "Installing collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 2.4.5\n",
      "    Uninstalling pyspark-2.4.5:\n",
      "      Successfully uninstalled pyspark-2.4.5\n",
      "Successfully installed pyspark-2.4.5\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=pyspark-shell>\n",
      "<pyspark.sql.session.SparkSession object at 0x7f6edc2dfda0>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "import pyspark\n",
    "\n",
    "number_cores = 4\n",
    "memory_gb = 16\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster('local[{}]'.format(number_cores))\n",
    "        .set('spark.driver.memory', '{}g'.format(memory_gb))\n",
    ")\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "print(sc)\n",
    "\n",
    "# get the context\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "print(spark) \n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('u.data', sep='\\t', header=None, engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+---------+\n",
      "|user_id|item_id|rating|timestamp|\n",
      "+-------+-------+------+---------+\n",
      "|    196|    242|     3|881250949|\n",
      "|    186|    302|     3|891717742|\n",
      "|     22|    377|     1|878887116|\n",
      "|    244|     51|     2|880606923|\n",
      "|    166|    346|     1|886397596|\n",
      "|    298|    474|     4|884182806|\n",
      "|    115|    265|     2|881171488|\n",
      "|    253|    465|     5|891628467|\n",
      "|    305|    451|     3|886324817|\n",
      "|      6|     86|     3|883603013|\n",
      "|     62|    257|     2|879372434|\n",
      "|    286|   1014|     5|879781125|\n",
      "|    200|    222|     5|876042340|\n",
      "|    210|     40|     3|891035994|\n",
      "|    224|     29|     3|888104457|\n",
      "|    303|    785|     3|879485318|\n",
      "|    122|    387|     5|879270459|\n",
      "|    194|    274|     2|879539794|\n",
      "|    291|   1042|     4|874834944|\n",
      "|    234|   1184|     2|892079237|\n",
      "+-------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=sqlContext.createDataFrame(df)\n",
    "df=df.withColumnRenamed('0','user_id')\n",
    "df=df.withColumnRenamed('1','item_id')\n",
    "df=df.withColumnRenamed('2','rating')\n",
    "df=df.withColumnRenamed('3','timestamp')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- item_id: long (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 4)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(rating)|\n",
      "+-----------+\n",
      "|    3.52986|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Calculate average movie rating\n",
    "df.agg(F.mean('rating')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- item_id: long (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n",
      "79986\n"
     ]
    }
   ],
   "source": [
    "# Test Train Split\n",
    "df.createOrReplaceTempView('df')\n",
    "(training, test) = df.randomSplit([0.8, 0.2])\n",
    "training.printSchema() \n",
    "print(training.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:0.9149176765073985\n",
      "Rank: 30\n",
      "MaxIter: 7\n",
      "RegParam: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rating\",\n",
    "         coldStartStrategy=\"drop\", nonnegative = True)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(als.regParam, [0.03,0.1,0.3]) \\\n",
    "  .addGrid(als.maxIter,[3,5,7]) \\\n",
    "  .addGrid(als.rank, [3,10,30]).build()\n",
    "\n",
    "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=als,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=regEval)\n",
    "\n",
    "model = tvs.fit(training)\n",
    "\n",
    "best_model = model.bestModel\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = best_model.transform(test)\n",
    "rmse = regEval.evaluate(predictions)\n",
    "\n",
    "print(\"RMSE:\"+str(rmse))\n",
    "print(\"Rank:\", best_model.rank)\n",
    "print(\"MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
    "print(\"RegParam:\", best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Flow\n",
    "from collections import deque\n",
    "from six import next\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 4)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_df = pd.read_table('u.data', sep='\\t', header=None, engine = 'python')\n",
    "ts_df.rename(columns={0:'user_id',1:'item_id',2:'rating',3:'timestamp'},inplace=True)\n",
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ts_df.drop('rating',axis=1)\n",
    "y = pd.DataFrame(ts_df['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Recommenders'\n",
      "/project\n"
     ]
    }
   ],
   "source": [
    "#pip install -e git+https://github.com/microsoft/recommenders/#egg=pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from reco_utils.common.general_utils import get_number_processors\n",
    "from reco_utils.common.gpu_utils import get_cuda_version, get_cudnn_version\n",
    "from reco_utils.dataset.python_splitters import python_stratified_split\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant seed for replicating training results\n",
    "np.random.seed(42)\n",
    "\n",
    "u_num = 6040 # Number of users in the dataset\n",
    "i_num = 3952 # Number of movies in the dataset\n",
    "\n",
    "batch_size = 1000 # Number of samples per batch\n",
    "dims = 5          # Dimensions of the data, 15\n",
    "max_epochs = 50   # Number of times the network sees all the training data\n",
    "\n",
    "# Device used for all computations\n",
    "place_device = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    ts_df = pd.read_table('u.data', sep='\\t', header=None, engine = 'python')\n",
    "    ts_df.rename(columns={0:'user_id',1:'item_id',2:'rating',3:'timestamp'},inplace=True)\n",
    "    rows = len(ts_df)\n",
    "    # Purely integer-location based indexing for selection by position\n",
    "    ts_df = ts_df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    # Separate data into train and test, 90% for train and 10% for test\n",
    "    split_index = int(rows * 0.9)\n",
    "    # Use indices to separate the data\n",
    "    df_train = ts_df[0:split_index]\n",
    "    df_test = ts_df[split_index:].reset_index(drop=True)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(user_batch, item_batch, user_num, item_num, dim=5, device=\"/cpu:0\"):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # Using a global bias term\n",
    "        bias_global = tf.get_variable(\"bias_global\", shape=[])\n",
    "        # User and item bias variables\n",
    "        # get_variable: Prefixes the name with the current variable scope \n",
    "        # and performs reuse checks.\n",
    "        w_bias_user = tf.get_variable(\"embd_bias_user\", shape=[user_num])\n",
    "        w_bias_item = tf.get_variable(\"embd_bias_item\", shape=[item_num])\n",
    "        # embedding_lookup: Looks up 'ids' in a list of embedding tensors\n",
    "        # Bias embeddings for user and items, given a batch\n",
    "        bias_user = tf.nn.embedding_lookup(w_bias_user, user_batch, name=\"bias_user\")\n",
    "        bias_item = tf.nn.embedding_lookup(w_bias_item, item_batch, name=\"bias_item\")\n",
    "        # User and item weight variables\n",
    "        w_user = tf.get_variable(\"embd_user\", shape=[user_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        w_item = tf.get_variable(\"embd_item\", shape=[item_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        # Weight embeddings for user and items, given a batch\n",
    "        embd_user = tf.nn.embedding_lookup(w_user, user_batch, name=\"embedding_user\")\n",
    "        embd_item = tf.nn.embedding_lookup(w_item, item_batch, name=\"embedding_item\")\n",
    "    \n",
    "    with tf.device(device):\n",
    "        # reduce_sum: Computes the sum of elements across dimensions of a tensor\n",
    "        infer = tf.reduce_sum(tf.multiply(embd_user, embd_item), 1)\n",
    "        infer = tf.add(infer, bias_global)\n",
    "        infer = tf.add(infer, bias_user)\n",
    "        infer = tf.add(infer, bias_item, name=\"svd_inference\")\n",
    "        # l2_loss: Computes half the L2 norm of a tensor without the sqrt\n",
    "        regularizer = tf.add(tf.nn.l2_loss(embd_user), tf.nn.l2_loss(embd_item), \n",
    "                             name=\"svd_regularizer\")\n",
    "    return infer, regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(infer, regularizer, rate_batch, learning_rate=0.1, reg=0.1, device=\"/cpu:0\"):\n",
    "    with tf.device(device):\n",
    "        # Use L2 loss to compute penalty\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
    "        penalty = tf.constant(reg, dtype=tf.float32, shape=[], name=\"l2\")\n",
    "        cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))\n",
    "        # 'Follow the Regularized Leader' optimizer\n",
    "        # Reference: http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf\n",
    "        train_op = tf.train.FtrlOptimizer(learning_rate).minimize(cost)\n",
    "    return cost, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples 90000, test samples 10000, samples per batch 90\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = get_data()\n",
    "\n",
    "samples_per_batch = len(df_train) // batch_size\n",
    "print(\"Number of train samples %d, test samples %d, samples per batch %d\" % \n",
    "      (len(df_train), len(df_test), samples_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    877\n",
      "1    815\n",
      "2     94\n",
      "3    416\n",
      "4    500\n",
      "Name: user_id, dtype: int64\n",
      "0    655\n",
      "1    178\n",
      "2    873\n",
      "3     10\n",
      "4    325\n",
      "Name: user_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"user_id\"].head()) \n",
    "print(df_test[\"user_id\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    381\n",
      "1    602\n",
      "2    431\n",
      "3    875\n",
      "4    182\n",
      "Name: item_id, dtype: int64\n",
      "0    271\n",
      "1     83\n",
      "2    294\n",
      "3    116\n",
      "4    305\n",
      "Name: item_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"item_id\"].head())\n",
    "print(df_test[\"item_id\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a shuffle iterator to generate random batches, for training\n",
    "iter_train = readers.ShuffleIterator([df_train[\"user_id\"],\n",
    "                                     df_train[\"item_id\"],\n",
    "                                     df_train[\"rating\"]],\n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "# Sequentially generate one-epoch batches, for testing\n",
    "iter_test = readers.OneEpochIterator([df_test[\"user_id\"],\n",
    "                                     df_test[\"item_id\"],\n",
    "                                     df_test[\"rating\"]],\n",
    "                                     batch_size=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "infer, regularizer = model(user_batch, item_batch, user_num=u_num, item_num=i_num, dim=dims, device=place_device)\n",
    "_, train_op = loss(infer, regularizer, rate_batch, learning_rate=0.10, reg=0.05, device=place_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tTrain Error\tVal Error\tElapsed Time\n",
      "00\t2.376\t\t2.167\t\t0.065 secs\n",
      "01\t1.140\t\t0.960\t\t0.103 secs\n",
      "02\t0.917\t\t0.937\t\t0.097 secs\n",
      "03\t0.892\t\t0.925\t\t0.094 secs\n",
      "04\t0.874\t\t0.919\t\t0.097 secs\n",
      "05\t0.861\t\t0.914\t\t0.096 secs\n",
      "06\t0.855\t\t0.911\t\t0.097 secs\n",
      "07\t0.854\t\t0.910\t\t0.097 secs\n",
      "08\t0.844\t\t0.907\t\t0.095 secs\n",
      "09\t0.839\t\t0.907\t\t0.094 secs\n",
      "10\t0.840\t\t0.906\t\t0.096 secs\n",
      "11\t0.834\t\t0.905\t\t0.095 secs\n",
      "12\t0.826\t\t0.905\t\t0.095 secs\n",
      "13\t0.826\t\t0.904\t\t0.094 secs\n",
      "14\t0.824\t\t0.904\t\t0.096 secs\n",
      "15\t0.817\t\t0.904\t\t0.095 secs\n",
      "16\t0.817\t\t0.904\t\t0.092 secs\n",
      "17\t0.815\t\t0.904\t\t0.097 secs\n",
      "18\t0.814\t\t0.904\t\t0.095 secs\n",
      "19\t0.814\t\t0.904\t\t0.093 secs\n",
      "20\t0.811\t\t0.904\t\t0.096 secs\n",
      "21\t0.809\t\t0.905\t\t0.095 secs\n",
      "22\t0.805\t\t0.905\t\t0.094 secs\n",
      "23\t0.809\t\t0.904\t\t0.098 secs\n",
      "24\t0.803\t\t0.904\t\t0.093 secs\n",
      "25\t0.806\t\t0.904\t\t0.096 secs\n",
      "26\t0.804\t\t0.905\t\t0.094 secs\n",
      "27\t0.800\t\t0.905\t\t0.093 secs\n",
      "28\t0.803\t\t0.905\t\t0.096 secs\n",
      "29\t0.800\t\t0.906\t\t0.096 secs\n",
      "30\t0.799\t\t0.906\t\t0.094 secs\n",
      "31\t0.801\t\t0.906\t\t0.098 secs\n",
      "32\t0.798\t\t0.906\t\t0.095 secs\n",
      "33\t0.793\t\t0.906\t\t0.096 secs\n",
      "34\t0.797\t\t0.907\t\t0.097 secs\n",
      "35\t0.802\t\t0.907\t\t0.095 secs\n",
      "36\t0.799\t\t0.907\t\t0.095 secs\n",
      "37\t0.794\t\t0.907\t\t0.096 secs\n",
      "38\t0.791\t\t0.907\t\t0.096 secs\n",
      "39\t0.792\t\t0.906\t\t0.094 secs\n",
      "40\t0.791\t\t0.906\t\t0.097 secs\n",
      "41\t0.795\t\t0.906\t\t0.096 secs\n",
      "42\t0.792\t\t0.906\t\t0.096 secs\n",
      "43\t0.788\t\t0.906\t\t0.094 secs\n",
      "44\t0.792\t\t0.906\t\t0.094 secs\n",
      "45\t0.789\t\t0.906\t\t0.094 secs\n",
      "46\t0.789\t\t0.906\t\t0.097 secs\n",
      "47\t0.784\t\t0.906\t\t0.107 secs\n",
      "48\t0.791\t\t0.907\t\t0.095 secs\n",
      "49\t0.790\t\t0.907\t\t0.095 secs\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" %(\"Epoch\", \"Train Error\", \"Val Error\", \"Elapsed Time\"))\n",
    "    errors = deque(maxlen=samples_per_batch)\n",
    "    start = time.time()\n",
    "    for i in range(max_epochs*samples_per_batch):\n",
    "        users, items, rates = next(iter_train)\n",
    "        _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch : users,\n",
    "                                                               item_batch : items, \n",
    "                                                               rate_batch : rates})\n",
    "        pred_batch = clip(pred_batch)\n",
    "        errors.append(np.power(pred_batch - rates,2))\n",
    "        if i % samples_per_batch == 0:\n",
    "            train_err = np.sqrt(np.mean(errors))\n",
    "            test_err2 = np.array([])\n",
    "            for users, items, rates in iter_test:\n",
    "                pred_batch = sess.run(infer, feed_dict={user_batch : users,\n",
    "                                                        item_batch : items})\n",
    "                pred_batch = clip(pred_batch)\n",
    "                test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "            end = time.time()\n",
    "            print(\"%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\" % (i // samples_per_batch, train_err, np.sqrt(np.mean(test_err2)), end - start))\n",
    "            start = end\n",
    "\n",
    "    saver.save(sess, './save/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda/envs/Python3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./save/model\n",
      "Pred\tActual\n",
      "2.426\t3.000\n",
      "3.389\t5.000\n",
      "3.669\t3.000\n",
      "3.693\t4.000\n",
      "4.372\t5.000\n",
      "3.950\t5.000\n",
      "2.730\t2.000\n",
      "3.791\t4.000\n",
      "3.387\t3.000\n",
      "3.750\t5.000\n",
      "0.9071531696822109\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(init_op)\n",
    "    new_saver = tf.train.import_meta_graph('./save/model.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./save/'))\n",
    "    test_err2 = np.array([])\n",
    "    for users, items, rates in iter_test:\n",
    "        pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                item_batch: items})\n",
    "        pred_batch = clip(pred_batch)\n",
    "        print(\"Pred\\tActual\")\n",
    "        for ii in range(10):\n",
    "            print(\"%.3f\\t%.3f\" % (pred_batch[ii], rates[ii]))\n",
    "        test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "        print(\"RMSE:\",np.sqrt(np.mean(test_err2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (5.3)\n",
      "Requirement already satisfied: h5py in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (1.14.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from keras) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0      196      242       3\n",
       "1      186      302       3\n",
       "2       22      377       1\n",
       "3      244       51       2\n",
       "4      166      346       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "nn_df = pd.read_table('u.data', sep='\\t', header=None, engine = 'python')\n",
    "nn_df.rename(columns={0:'user_id',1:'item_id',2:'rating',3:'timestamp'},inplace=True)\n",
    "num_users = nn_df['user_id'].max()\n",
    "num_items = nn_df['item_id'].max()\n",
    "nn_df = nn_df.drop('timestamp', 1)\n",
    "nn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling data\n",
    "shuffled_df = nn_df.sample(frac=1., random_state=42)\n",
    "users = shuffled_df['user_id'].values\n",
    "items = shuffled_df['item_id'].values\n",
    "ratings = shuffled_df['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Embedding, Reshape, Dropout, Dense, merge, Input, Flatten, Concatenate, Dot, Add, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = Sequential()\n",
    "U.add(Embedding(num_users, embeddings_size, input_length=1))\n",
    "U.add(Reshape((embeddings_size,)))\n",
    "I = Sequential()\n",
    "I.add(Embedding(num_items, embeddings_size, input_length=1))\n",
    "I.add(Reshape((embeddings_size,)))\n",
    "M = Sequential()\n",
    "M.add(Concatenate([U, I]))\n",
    "M.add(Dot(1))\n",
    "M.add(Dropout(0.2))\n",
    "M.add(Dense(1, activation='relu'))\n",
    "M.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping('val_loss', patience=2), \n",
    "             ModelCheckpoint('./data/weights.h5', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.astype('float')\n",
    "items = items.astype('float')\n",
    "ratings = ratings.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-aef1dd954510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = M.fit(x=[users, items],y=ratings,\n\u001b[1;32m      2\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                 batch_size=128)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0;31m# to match the value shapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;31m# since `Sequential` depends on `Model`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = M.fit(x=[users, items],y=ratings,\n",
    "                epochs=20, validation_split=.1, \n",
    "                batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-965cad2b9b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m results = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n\u001b[0m\u001b[1;32m      2\u001b[0m                      \u001b[0;34m'training'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                      'validation': [ math.sqrt(loss) for loss in history.history['val_loss'] ]})\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RMSE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
    "                     'training': [ math.sqrt(loss) for loss in history.history['loss'] ],\n",
    "                     'validation': [ math.sqrt(loss) for loss in history.history['val_loss'] ]})\n",
    "ax = results.ix[:,:].plot(x='epoch')\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.set_ylim([0.0,3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling data\n",
    "shuffled_df = nn_df.sample(frac=1., random_state=42)\n",
    "users = shuffled_df['user_id'].values\n",
    "items = shuffled_df['item_id'].values\n",
    "ratings = shuffled_df['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = shuffled_df.user_id.max() + 1 # number of users\n",
    "M = shuffled_df.item_id.max() + 1 # number of movies\n",
    "\n",
    "# initialize variables\n",
    "K = 10 # latent dimensionality\n",
    "mu = shuffled_df.rating.mean()\n",
    "epochs = 15\n",
    "reg = 0. # regularization penalty\n",
    "\n",
    "\n",
    "# keras model\n",
    "u = Input(shape=(1,))\n",
    "m = Input(shape=(1,))\n",
    "u_embedding = Embedding(N, K)(u) # (N, 1, K)\n",
    "m_embedding = Embedding(M, K)(m) # (N, 1, K)\n",
    "\n",
    "\n",
    "##### main branch\n",
    "u_bias = Embedding(N, 1)(u) # (N, 1, 1)\n",
    "m_bias = Embedding(M, 1)(m) # (N, 1, 1)\n",
    "x = Dot(axes=2)([u_embedding, m_embedding]) # (N, 1, 1)\n",
    "x = Add()([x, u_bias, m_bias])\n",
    "x = Flatten()(x) # (N, 1)\n",
    "\n",
    "\n",
    "##### side branch\n",
    "u_embedding = Flatten()(u_embedding) # (N, K)\n",
    "m_embedding = Flatten()(m_embedding) # (N, K)\n",
    "y = Concatenate()([u_embedding, m_embedding]) # (N, 2K)\n",
    "y = Dense(400)(y)\n",
    "y = Activation('elu')(y)\n",
    "# y = Dropout(0.5)(y)\n",
    "y = Dense(1)(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "90000/90000 [==============================] - 1s 14us/step - loss: 0.9844 - mse: 0.9844 - val_loss: 0.9140 - val_mse: 0.9140\n",
      "Epoch 2/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.8907 - mse: 0.8907 - val_loss: 0.9135 - val_mse: 0.9135\n",
      "Epoch 3/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.8726 - mse: 0.8726 - val_loss: 0.8967 - val_mse: 0.8967\n",
      "Epoch 4/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.8538 - mse: 0.8538 - val_loss: 0.8718 - val_mse: 0.8718\n",
      "Epoch 5/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.8377 - mse: 0.8377 - val_loss: 0.8748 - val_mse: 0.8748\n",
      "Epoch 6/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.8666 - val_mse: 0.8666\n",
      "Epoch 7/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.8125 - mse: 0.8125 - val_loss: 0.8502 - val_mse: 0.8502\n",
      "Epoch 8/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.8011 - mse: 0.8011 - val_loss: 0.8512 - val_mse: 0.8512\n",
      "Epoch 9/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.7920 - mse: 0.7920 - val_loss: 0.8465 - val_mse: 0.8465\n",
      "Epoch 10/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.7789 - mse: 0.7789 - val_loss: 0.8638 - val_mse: 0.8638\n",
      "Epoch 11/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.7638 - mse: 0.7638 - val_loss: 0.8448 - val_mse: 0.8448\n",
      "Epoch 12/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.7473 - mse: 0.7473 - val_loss: 0.8417 - val_mse: 0.8417\n",
      "Epoch 13/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.7277 - mse: 0.7277 - val_loss: 0.8419 - val_mse: 0.8419\n",
      "Epoch 14/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.7076 - mse: 0.7076 - val_loss: 0.8446 - val_mse: 0.8446\n",
      "Epoch 15/15\n",
      "90000/90000 [==============================] - 1s 12us/step - loss: 0.6858 - mse: 0.6858 - val_loss: 0.8586 - val_mse: 0.8586\n"
     ]
    }
   ],
   "source": [
    "##### merge\n",
    "x = Add()([x, y])\n",
    "\n",
    "model = Model(inputs=[u, m], outputs=x)\n",
    "model.compile(\n",
    "  loss='mse',\n",
    "  # optimizer='adam',\n",
    "  # optimizer=Adam(lr=0.01),\n",
    "  optimizer=SGD(lr=0.08, momentum=0.9),\n",
    "  metrics=['mse'],\n",
    ")\n",
    "\n",
    "r = model.fit(\n",
    "  x=[users, items],y=ratings - mu,\n",
    "  epochs=epochs,\n",
    "  batch_size=128,\n",
    "  validation_split=.1\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_mse', 'loss', 'mse'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:0.805\n"
     ]
    }
   ],
   "source": [
    "print('MSE:{:.3f}'.format(np.mean(r.history['mse'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 3.0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame({'epoch': [ i + 1 for i in r.epoch ],\n",
    "                     'training': [ math.sqrt(loss) for loss in r.history['loss'] ],\n",
    "                     'validation': [ math.sqrt(loss) for loss in r.history['val_loss'] ]})\n",
    "ax = results.iloc[:,:].plot(x='epoch')\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.set_ylim([0.0,3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
