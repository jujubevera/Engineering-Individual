{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using ALS with Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7wUqfSl2VmcX"
      },
      "source": [
        "# Using ALS with Spark \n",
        "\n",
        "This basic Recommender System will use the Spark ALS and explore the effects of outcome of a cross-validation on the dataset. \n",
        "This Notebook requires the Movielens dataset to be present in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3yhFTvcOVtHP",
        "outputId": "6834d47c-dc7a-41e2-b210-dec185e6fa71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkXWGGUJVFTV",
        "colab_type": "code",
        "outputId": "50c0eb7e-15ff-47d7-e4c8-dcbf97a6a12b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q http://www-eu.apache.org/dist/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "#!pip install pyspark\n",
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "\n",
        "!java -version\n",
        "!python --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_232\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_232-8u232-b09-0ubuntu1~18.04.1-b09)\n",
            "OpenJDK 64-Bit Server VM (build 25.232-b09, mixed mode)\n",
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJbOUFfacKqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"/content/spark-2.4.4-bin-hadoop2.7\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oZPg-wnZV0R-",
        "outputId": "7454c87e-23ba-4387-ac94-4e179c86799e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "  #install spark\n",
        "  #!pip install pyspark\n",
        "  import pyspark\n",
        "  # get a spark context\n",
        "  sc = pyspark.SparkContext.getOrCreate()\n",
        "  print(sc)\n",
        "  # and a spark session\n",
        "  spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
        "  print(spark)\n",
        "  spark.version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SparkContext master=local[*] appName=pyspark-shell>\n",
            "<pyspark.sql.session.SparkSession object at 0x7f584b0febe0>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.4.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhbRbBLCWD1h",
        "colab_type": "code",
        "outputId": "297a8b74-e43c-48e8-ddbf-bb0480380d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "source": [
        "from google.colab import files\n",
        "!ls -l\n",
        "#!rm spark-2.4.4-bin-hadoop2.7.tgz.1\n",
        "\n",
        "!echo $JAVA_HOME/bin\n",
        "!export PATH=$PATH:$JAVA_HOME/bin\n",
        "!echo $PATH"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 449412\n",
            "drwx------  4 root root      4096 Dec 27 12:17 drive\n",
            "drwxr-xr-x  1 root root      4096 Dec 18 16:52 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Aug 27 21:30 spark-2.4.4-bin-hadoop2.7\n",
            "-rw-r--r--  1 root root 230091034 Aug 27 22:01 spark-2.4.4-bin-hadoop2.7.tgz\n",
            "-rw-r--r--  1 root root 230091034 Aug 27 22:01 spark-2.4.4-bin-hadoop2.7.tgz.1\n",
            "/usr/lib/jvm/java-8-openjdk-amd64/bin\n",
            "/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xaKVofYd95G",
        "colab_type": "text"
      },
      "source": [
        "check versions and dependencies, this may change with changes to Colab and Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocjx7B2EXPL3",
        "colab_type": "code",
        "outputId": "26765894-6dce-4229-a745-1b221860f847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        }
      },
      "source": [
        "!java -version\n",
        "!python --version\n",
        "!ls /usr/lib/jvm/\n",
        "!echo $JAVA_HOME\n",
        "spark.version\n",
        "!ls /content\n",
        "!echo $PATH"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_232\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_232-8u232-b09-0ubuntu1~18.04.1-b09)\n",
            "OpenJDK 64-Bit Server VM (build 25.232-b09, mixed mode)\n",
            "Python 3.6.9\n",
            "default-java\t\t   java-11-openjdk-amd64     java-8-openjdk-amd64\n",
            "java-1.11.0-openjdk-amd64  java-1.8.0-openjdk-amd64\n",
            "/usr/lib/jvm/java-8-openjdk-amd64\n",
            "drive\t     spark-2.4.4-bin-hadoop2.7\t    spark-2.4.4-bin-hadoop2.7.tgz.1\n",
            "sample_data  spark-2.4.4-bin-hadoop2.7.tgz\n",
            "/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Da0U58DFVmcb"
      },
      "source": [
        "## Step 1 - Load the Data\n",
        "Read the data, split into tokens and create a structured DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zE39AsRoVmcf",
        "outputId": "b5745ce2-e475-4d02-80c7-6ba7ea7e5811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType\n",
        "# the imports are used creating the data frame\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate() # create a SparkSession \n",
        "# this gets us an RDD. (could also be done with RDD.textFile in this case)\n",
        "lines = spark.read.text(\"/content/drive/My Drive/Colab Notebooks/data/movielens-small/sample_movielens_ratings.txt\").rdd\n",
        "# now split the lines at the '::'\n",
        "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
        "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
        "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
        "ratings = spark.createDataFrame(ratingsRDD)\n",
        "ratings.createOrReplaceTempView('ratings') # register the DataFrame so that we can use it with Spark SQL.\n",
        "(training, test) = ratings.randomSplit([0.8, 0.2]) # split into test and training set\n",
        "training.printSchema() # just for testing, should show the four columns\n",
        "print(training.count()) # just for testing, should be around 1200"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- movieId: long (nullable = true)\n",
            " |-- rating: double (nullable = true)\n",
            " |-- timestamp: long (nullable = true)\n",
            " |-- userId: long (nullable = true)\n",
            "\n",
            "1213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uGJDZMctVmcv"
      },
      "source": [
        "# Step 2 - Create a baseline\n",
        "\n",
        "Now take a very simple estimate as the baseline: calculate the mean of all ratings.    \n",
        "\n",
        "The average can be calculated with the SQL `AVG` command, within an SQL `SELECT` statement. \n",
        "\n",
        "Then calculate the MSE with respect to the average (as predictor)\n",
        "\n",
        "Calculate the RMSE as a naive baseline to compare the trained model against\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k9FB2exsVmcx",
        "outputId": "925e02f8-4f67-43fb-bc5f-e16888c4475c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "SQL1 = 'SELECT AVG(rating) FROM ratings'\n",
        "row = spark.sql(SQL1).collect()[0] # get the single row with the result\n",
        "\n",
        "meanRating = row['avg(rating)'] # access Row as a map \n",
        "print('meanRating',meanRating)\n",
        "\n",
        "se_rdd = test.rdd.map(lambda row: Row(se = pow(row['rating']-meanRating,2)) ) \n",
        "se_df = spark.createDataFrame(se_rdd) \n",
        "se_df.createOrReplaceTempView('se')\n",
        "print('se_df',se_df)\n",
        "SQL2 = 'SELECT AVG(se) FROM se'\n",
        "row = spark.sql(SQL2).collect()[0]\n",
        "meanSE = row['avg(se)'] # access Row as a map \n",
        "print('RMSE',pow(meanSE,0.5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "meanRating 1.7741505662891406\n",
            "se_df DataFrame[se: double]\n",
            "RMSE 1.142816731411422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BcvJZND5Vmc4"
      },
      "source": [
        "## Step 3 - Train an ALS Estimator and perform CV \n",
        "\n",
        "Now create an ALS estimator and a parameter grid to explore different values for the `rank` and `regParam` parameter of the ALS. Then build a cross-validator to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zpPgO80HVmc6",
        "outputId": "9636c8f8-7113-4d80-ead7-410c21ef0a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "# Build the recommendation model using ALS on the training data\n",
        "als = ALS(maxIter=3, rank=10, regParam=0.1, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "  .addGrid(als.regParam, [0.03,0.1,0.3]) \\\n",
        "  .addGrid(als.rank, [3,10,30]).build()\n",
        "\n",
        "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "\n",
        "crossVal = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=regEval)\n",
        "print('starting cross-validation')\n",
        "cvModel = crossVal.fit(training)\n",
        "print('finished cross-validation')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting cross-validation\n",
            "finished cross-validation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JTvVhTN3Vmc_"
      },
      "source": [
        "## Step 4 - Evaluate The Model\n",
        "\n",
        "Take the trained cvModel and extract the best parameter values by inspecting the estimatorParameterMaps. \n",
        "Compare the RMSE value to that of the mean for different parameter settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D0ygAMOEVmdA",
        "outputId": "bd8f62ae-1963-4532-bfce-73233e3f24e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "print(cvModel.avgMetrics) # the metrics form the CrossValidation\n",
        "print(cvModel.getEstimatorParamMaps()) # gives you the parameter combinations\n",
        "paramMap = list(zip(cvModel.getEstimatorParamMaps(),cvModel.avgMetrics))\n",
        "paramMin = min(paramMap, key=lambda x: x[1])\n",
        "print(paramMin)\n",
        "\n",
        "# Evaluate the model by computing the RMSE on the test data\n",
        "predictions = cvModel.transform(test)\n",
        "rmse = regEval.evaluate(predictions)\n",
        "print(\"RMSE = \" + str(rmse))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.5254898760653246, 2.0261790089927163, 1.57165590851438, 1.3294585606453688, 1.4275271366035416, 1.2792006504743827, 1.2335536001799277, 1.2394600343653657, 1.226236628398472]\n",
            "[{Param(parent='ALS_0217f748ceb9', name='regParam', doc='regularization parameter (>= 0).'): 0.03, Param(parent='ALS_0217f748ceb9', name='rank', doc='rank of the factorization'): 3}, {Param(parent='ALS_0217f748ceb9', name='regParam', doc='regularization parameter (>= 0).'): 0.03, Param(parent='ALS_0217f748ceb9', name='rank', doc='rank of the factorization'): 10}, {Param(parent='ALS_0217f748ceb9', name='regParam', doc='regularization parameter (>= 0).'): 0.03, Param(parent='ALS_0217f748ceb9', name='rank', doc='rank of the factorization'): 30}, {Param(parent='ALS_0217f748ceb9', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='ALS_0217f748ceb9', name='rank', doc='rank of the factorization'): 3}, {Param(parent='ALS_0217f748ceb9', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='ALS_0217f748ceb9', name='rank', doc='rank of the factorization'): 10}, {Param(parent='ALS_0217f748ceb9', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='ALS_0217f748ceb9', name='rank', doc='rank of the factorization'): 30}, {Param(parent='ALS_0217f748ceb9', name='regParam', doc='regularization parameter (>= 0).'): 0.3, Param(parent='ALS_0217f748ceb9', name='rank', doc='rank of the factorization'): 3}, {Param(parent='ALS_0217f748ceb9', name='regParam', doc='regularization parameter (>= 0).'): 0.3, Param(parent='ALS_0217f748ceb9', name='rank', doc='rank of the factorization'): 10}, {Param(parent='ALS_0217f748ceb9', name='regParam', doc='regularization parameter (>= 0).'): 0.3, Param(parent='ALS_0217f748ceb9', name='rank', doc='rank of the factorization'): 30}]\n",
            "({Param(parent='ALS_0217f748ceb9', name='regParam', doc='regularization parameter (>= 0).'): 0.3, Param(parent='ALS_0217f748ceb9', name='rank', doc='rank of the factorization'): 30}, 1.226236628398472)\n",
            "RMSE = 1.0723709479269128\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}